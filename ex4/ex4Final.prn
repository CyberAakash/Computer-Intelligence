Script started on Tue 22 Aug 2023 11:25:04 AM IST
[20bcs025@mepcolinux ex4]$cat Decision_tree.py 
import math
import pandas as pd

class DecisionTree:
    def calc_entropy(self, splits):
        total_count = sum(splits)
        if total_count == 0:
            return 0
        entropy = 0
        for split in splits:
            if split > 0:
                prob = split / total_count
                entropy -= prob * math.log2(prob)
        return round(entropy, 4)

    def info_gain(self, column, total_entropy):
        gain = 0
        for value in self.x_data[column].unique():
            subdata = self.data[self.x_data[column] == value]
            if self.y_data_dtype == 'object':
                entropy_splits = [sum(subdata[self.y_column].str.count(label)) for label in self.y_unique_labels]
            else:
                entropy_splits = [sum(subdata[self.y_column] == label) for label in self.y_unique_labels]

            print(f"Splits {entropy_splits}")

            if self.y_data_dtype == 'object':
                entropy = self.calc_entropy(entropy_splits)
            else:
                entropy = self.calc_entropy(entropy_splits)

            print(f"Entropy({value}) = {entropy}")

            gain += (len(subdata) / self.total_records) * entropy

        print(f"Total weighted entropy for {column} : {round(gain, 5)}")
        return round(total_entropy - gain, 5)

    def find_root(self, x_data, y_data):
        self.x_data = x_data
        self.y_data = y_data
        self.data = pd.concat([x_data, y_data], axis=1)
        self.total_records = len(x_data)
        self.y_column = y_data.columns[0]
        self.y_unique_labels = y_data[self.y_column].unique()
        self.y_data_dtype = y_data[self.y_column].dtype

        total_entropy_splits = [sum(y_data[self.y_column] == label) for label in self.y_unique_labels]
        total_entropy = self.calc_entropy(total_entropy_splits)

        print(f"Splits {total_entropy_splits}")
        print(f"Entropy(S) = {total_entropy}\n\n==================================\n")

        max_gain = -1
        root = None
        for column in x_data.columns:
            print(f"Feature = {column}\n")
            gain = self.info_gain(column, total_entropy)
            if gain > max_gain:
                max_gain = gain
                root = column
            print(f"\nInfo gain({column}) = {gain}\n\n==================================\n")

        print(f"Max gain = {max_gain}")
        return root

if __name__ == "__main__":
    filename = input("Enter the dataset filename: ")
    data = pd.read_csv(filename)
    x_data = data.iloc[:, 1:-1]
    y_data = data.iloc[:, -1:]
    model = DecisionTree()
    root_feature = model.find_root(x_data, y_data)
    print("Root feature =", root_feature)


[20bcs025@mepcolinux ex4]$python3 Decision_tree.py 
Enter the dataset filename: mam.csv
Splits [5, 9]
Entropy(S) = 0.9403

==================================

Feature =  age

Splits [3, 2]
Entropy( let) = 0.971
Splits [0, 4]
Entropy( tf ) = 0.0
Splits [1, 2]
Entropy( gef) = 0.9183
Splits [1, 1]
Entropy( gtf) = 1.0
Total weighted entropy for  age : 0.68642

Info gain( age) = 0.25388

==================================

Feature =  income

Splits [2, 3]
Entropy( high  ) = 0.971
Splits [2, 3]
Entropy( medium) = 0.971
Splits [1, 3]
Entropy( low   ) = 0.8113
Total weighted entropy for  income : 0.92537

Info gain( income) = 0.01493

==================================

Feature =  credit_rating

Splits [2, 6]
Entropy( fair         ) = 0.8113
Splits [3, 3]
Entropy( excellent    ) = 1.0
Total weighted entropy for  credit_rating : 0.89217

Info gain( credit_rating) = 0.04813

==================================

Max gain = 0.25388
Root feature =  age
[20bcs025@mepcolinux ex4]$exit
exit

Script done on Tue 22 Aug 2023 11:25:21 AM IST
